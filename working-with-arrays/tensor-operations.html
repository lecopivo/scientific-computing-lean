<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Tensor Operations - Scientific Computing in Lean</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../custom.css">
        <link rel="stylesheet" href="../pygments.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../title.html">Scientific Computing in Lean</a></li><li class="chapter-item expanded "><a href="../introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">1.1.</strong> Sorry Friendly Programming</div></li><li class="chapter-item expanded "><a href="../installation.html"><strong aria-hidden="true">1.2.</strong> Setting Up Lean and SciLean</a></li></ol></li><li class="chapter-item expanded "><a href="../working-with-arrays.html"><strong aria-hidden="true">2.</strong> Working with Arrays</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../working-with-arrays/basics.html"><strong aria-hidden="true">2.1.</strong> Basic Operations</a></li><li class="chapter-item expanded "><a href="../working-with-arrays/tensor-operations.html" class="active"><strong aria-hidden="true">2.2.</strong> Tensor Operations</a></li><li class="chapter-item expanded "><a href="../working-with-arrays/optimizing-arrays.html"><strong aria-hidden="true">2.3.</strong> Optimizing Array Expressions</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.4.</strong> Abstract Array Interface</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> Differentiation</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> Zoo of Differentiation Operators</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Defining New Functions</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.3.</strong> Theoretical Aspects*</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Function Transformations</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">4.1.</strong> Defining New Function Transformation</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.2.</strong> Function Transformation Algorithm*</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Problem Transformations</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">5.1.</strong> Solve Function</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.2.</strong> Approximation</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Differential Equations</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> Probabilistic Programming</div></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Scientific Computing in Lean</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/lecopivo/scientific-computing-lean" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="tensor-operations"><a class="header" href="#tensor-operations">Tensor Operations</a></h1>
<p>In this chapter, we will demonstrate more advanced operations with arrays, such as transformations and reductions. To provide a concrete example, we will build a simple neural network. It's important to note that Lean/SciLean is not yet suitable for running and training neural networks, as it only runs on CPU and the current compiler does not produce the most efficient code. Nevertheless, I believe that writing a simple neural network will nicely demonstrate Lean's expressivity. My secret hope is that this text will motivate someone to write a specialized compiler that will translate a subset of Lean to GPUs.</p>
<h2 id="transformations-and-reductions"><a class="header" href="#transformations-and-reductions">Transformations and Reductions</a></h2>
<p>One common operation is to transform every element of an array. To do that, we can write a simple for loop. Recall that anytime you want to write imperative-style code, you have to start it with <code>Id.run do</code>, and to modify input <code>x</code> mutably, we have to introduce a new mutable variable <code>x'</code> and assign <code>x</code> to it:</p>
<pre><code class="language-lean">def map {I : Type} [IndexType I] (x : Float^I) (f : Float → Float) := Id.run do
  let mut x' := x
  for i in IndexType.univ I do
    x'[i] := f x'[i]
  return x'
</code></pre>
<p>A new thing here is that we wrote this function polymorphically in the index type <code>I</code>. <code>{I : Type}</code> introduces a new type, and <code>[IndexType I]</code> adds a requirement that <code>I</code> behave as an index. <code>IndexType</code> is a type class that allows us to do a bunch of things with <code>I</code>. We have already seen <code>IndexType.card I</code>, which tells you the number of elements in <code>I</code>. There is also <code>IndexType.toFin</code> and <code>IndexType.fromFin</code>, which convert <code>i : I</code> into <code>toFin i : Fin (card I)</code> and <code>idx : Fin (card I)</code> to <code>fromFin idx : I</code>. So the function <code>toFin</code> allows you to linearize any index <code>I</code>, and it is the core function used to implement <code>DataArrayN</code>, as all elements of an array have to be stored linearly in memory.</p>
<p>In fact, SciLean already provides this function under the name <code>mapMono</code>. The &quot;mono&quot; stands for the fact that the function <code>f</code> does not change the type; in our case, it accepts and returns <code>Float</code>. Also, this function is defined in the <code>DataArrayN</code> namespace, and because of that, we can use the familiar dot notation <code>x.mapMono</code>. As <code>mapMono</code> is polymorphic in the shape of the array, we can call it on vectors:</p>
<pre><code class="language-lean">#eval ⊞[1.0,2.0,3.0].mapMono (fun x =&gt; sqrt x)
</code></pre>
<p>or matrices:</p>
<pre><code class="language-lean">#eval ⊞[1.0,2.0;3.0,4.0].mapMono (fun x =&gt; sqrt x)
</code></pre>
<p>or higher-rank arrays:</p>
<pre><code class="language-lean">#eval (⊞ (i j k : Fin 2) =&gt; (IndexType.toFin (i,j,k)).toFloat).mapMono (fun x =&gt; sqrt x)
</code></pre>
<p>where <code>IndexType.toFin (i,j,k)</code> turns a structured index of type <code>Fin 2 × Fin 2 × Fin 2</code> to a linear index of type <code>Fin 8</code>, <code>.toFloat</code> converts it to <code>Float</code>, and finally <code>.map (fun x =&gt; sqrt x)</code> computes the square root of every element.</p>
<p>An alternative to <code>mapMono</code> is <code>mapIdxMono</code>, which accepts a function <code>f : I → X → X</code>, so you can additionally use the index value to transform the array values:</p>
<pre><code class="language-lean">#eval (0 : Float^[3]) |&gt;.mapIdxMono (fun i _ =&gt; i.toFloat) |&gt;.map (fun x =&gt; sqrt x)
</code></pre>
<p>where <code>0 : Float^[3]</code> creates a zero array of size 3, then <code>mapIdxMono (fun i _ =&gt; i.toFloat)</code> initializes every element to the value of its index, and finally <code>map (fun x =&gt; sqrt x)</code> computes the square root of every element.</p>
<p>The next important operation with arrays is reduction, which runs over elements and reduces them using a provided binary operation. There are two main reductions, <code>x.fold op init</code> and <code>x.reduceD op default</code>. The difference is that <code>fold</code> uses <code>init</code> as the initial value that is updated with elements of the array <code>x</code>, while <code>reduceD</code> uses only the elements of <code>x</code> and returns <code>default</code> if <code>x</code> happens to be empty:</p>
<pre><code class="language-lean">x.fold op init = (op ... (op (op init x[0]) x[1]) ...n)
x.reduceD op default = (op ... (op (op x[0] x[1]) x[2]) ...)
</code></pre>
<p>There are also versions <code>x.reduce</code> where you do not have to provide the default element, but it is required that the element type <code>X</code> of the array <code>x : X^I</code> has an instance <code>Inhabited X</code>, which allows you to call <code>default : X</code>, returning a default element of <code>X</code>. For example, <code>default : Float</code> returns <code>0.0</code>.</p>
<p>To sum all elements of an array:</p>
<pre><code class="language-lean">#eval ⊞[1.0,2.0,3.0].fold (·+·) 0
</code></pre>
<p>or to find the minimal element:</p>
<pre><code class="language-lean">#eval ⊞[(1.0 :Float),2.0,3.0].reduce (min · ·)
</code></pre>
<p>notice that computing the minimal element with <code>fold</code> and <code>init:=0</code> would give you an incorrect answer.</p>
<p>Putting it all together we can implement soft-max</p>
<pre><code class="language-lean">def softMax {I} [IndexType I]
  (r : Float) (x : Float^I) : Float^I := Id.run do
  let m := x.reduce (max · ·)
  let x := x.map fun x =&gt; x-m
  let x := x.map fun x =&gt; exp r*x
  let w := x.reduce (·+·)
  let x := x.map fun x =&gt; x/w
  return x
</code></pre>
<p>where for numerical stability we first find the maximal element <code>m</code> and subtract it from all the element. After that we proceed with the standard definition of soft max. Of course, this is not the most efficient implementation of softmax. In later chapter, we will show how to transform it to a more efficient version.</p>
<p>Very common reduction is to sum element or to multiply them. <em>SciLean</em> provides familiar notation for these</p>
<pre><code>def x := ⊞[1.0,2.0,3.0,4.0]
def A := ⊞[1.0,2.0;3.0,4.0]

#eval ∑ i, x[i]
#eval ∏ i, x[i]
#eval ∑ i j, A[i,j]
#eval ∏ i j, A[i,j]
</code></pre>
<p><em>Note for Mathlib users: For performance reasons SciLean defines sums and products with <code>IndexType</code> instead of <code>Finset</code>. Therefore this notation is different from the one defined in <code>BigOperators</code> namespace.</em></p>
<p>We can define common matrix operations like matrix-vector multiplication</p>
<pre><code class="language-lean">def matMul {n m : Nat} (A : Float^[n,m]) (x : Float^[m]) :=
  ⊞ i =&gt; ∑ j, A[i,j] * x[j]
</code></pre>
<p>or trace</p>
<pre><code class="language-lean">def trace {n : Nat} (A : Float^[n,n]) :=
  ∑ i, A[i,i]
</code></pre>
<h2 id="convolution-and-operations-on-indices"><a class="header" href="#convolution-and-operations-on-indices">Convolution and Operations on Indices</a></h2>
<p>The fundamental operation in machine learning is convolution. The first attempt at writing convolution might look like this:</p>
<pre><code class="language-lean">def conv1d {n k : Nat} (x : Float^[n]) (w : Float^[k]) :=
    ⊞ (i : Fin n) =&gt; ∑ j, w[j] * x[i-j]
</code></pre>
<p>However, this produces an error:</p>
<pre><code class="language-lean">typeclass instance problem is stuck, it is often due to metavariables
  HSub (Fin n) (Fin k) ?m.48171
</code></pre>
<p>This error arises because Lean can't infer the subtraction operation between the types <code>Fin n</code> and <code>Fin k</code>, which would produce some unknown type <code>?m.48171</code>. This makes sense, what does it mean to subtract <code>j : Fin k</code> from <code>i : Fin n</code>? Because we are accessing elements of <code>x</code>, we probably want the result to be <code>Fin n</code>, but what do we do if <code>i - j</code> is smaller than zero? We need to do something more involved when performing operations on indices that have their range specified in their type.</p>
<p>Let's step back a bit and look at the type of the kernel <code>w : Float^[k]</code>. We index it with numbers <code>0,...,k-1</code>, but that is not how we usually think of kernels. We would rather index them by <code>-k,...,-1,0,1,...,k</code>. SciLean provides useful notation for this: <code>Float^[[-k:k]]</code>, which stands for <code>DataArrayN Float (Set.Icc (-k) k)</code> and <code>Set.Icc (-k) k</code> is a closed interval with endpoints <code>-k</code> and <code>k</code>. Because here we consider <code>k</code> to be an integer, then <code>Set.Icc (-k) k</code> is exactly the set of <code>-k,...,-1,0,1,...,k</code>. Recall that <code>i : Fin n</code> is a pair of the value <code>i.1 : ℕ</code> and proof <code>i.2 : i.1 &lt; n</code>. Similarly, <code>i : Set.Icc (-k) k</code> is a pair <code>i.1 : ℤ</code> and proof <code>i.2 : -k ≤ i.1 ∧ i.1 ≤ k</code>. The type for a two-dimensional kernel would be <code>Float^[[-k:k],[-k:k]]</code>, which stands for <code>DataArrayN Float (Set.Icc (-k) k × Set.Icc (-k) k)</code>.</p>
<p>Now, instead of writing <code>i-j</code>, we want to shift the index <code>i : Fin n</code> by the index <code>j</code> and obtain another index of type <code>Fin n</code>. Let's define a general function <code>shift</code> that shifts <code>Fin n</code> by an arbitrary integer:</p>
<pre><code class="language-lean">def Fin.shift {n} (i : Fin n) (j : ℤ) : Fin n :=
    { val := ((i.1 + j) % n ).toNat, isLt := sorry }
</code></pre>
<p>Here, <code>%</code> is already performing positive modulo on integers, and we again omitted the proof that the result is indeed smaller than <code>n</code>. It is not a hard proof, but the purpose of this text is not to teach you how to prove theorems in Lean but rather how to use Lean as a programming language, and omitting proofs is a perfectly valid approach.</p>
<p>Now we can write one-dimensional convolution as:</p>
<pre><code class="language-lean">def conv1d {n k : Nat} (w : Float^[[-k:k]]) (x : Float^[n]) :=
    ⊞ (i : Fin n) =&gt; ∑ j, w[j] * x[i.shift j.1]
</code></pre>
<p>This immediately generalizes to two dimensions:</p>
<pre><code class="language-lean">def conv2d {n m k : Nat} (w : Float^[[-k:k],[-k:k]]) (x : Float^[n,m]) :=
    ⊞ (i : Fin n) (j : Fin m) =&gt; ∑ a b, w[a,b] * x[i.shift a, j.shift b]
</code></pre>
<p>In practice, a convolutional layer takes as input a stack of images <code>x</code>, a stack of kernels <code>w</code>, and a bias <code>b</code>. Let's index images by an arbitrary type <code>I</code> and kernels by <code>J×I</code>:</p>
<pre><code class="language-lean">def conv2d {n m k : Nat} (J : Type) {I : Type} [IndexType I] [IndexType J] [DecidableEq J]
    (w : Float^[J,I,[-k:k],[-k:k]]) (b : Float^[J,n,m]) (x : Float^[I,n,m]) : Float^[J,n,m] :=
    ⊞ κ (i : Fin n) (j : Fin m) =&gt; b[κ,i,j] + ∑ ι a b, w[κ,ι,a,b] * x[ι, i.shift a, j.shift b]
</code></pre>
<h2 id="pooling-and-difficulties-with-dependent-types"><a class="header" href="#pooling-and-difficulties-with-dependent-types">Pooling and Difficulties with Dependent Types</a></h2>
<p>The next piece of neural networks is the pooling layer, a layer that reduces image resolution. Giving a good type to the pooling layer is quite challenging, as we have to divide the image resolution by two. Doing any kinds of operations in types brings out all the complexities of dependent type theory. Yes, dependent types can be really hard, but please do not get discouraged by this section. One has to be careful and not put dependent types everywhere, but when used with care, they can provide lots of benefits without causing too many troubles.</p>
<p>The canonical example is if you have an index <code>i : Fin (n + m)</code> and you have a function <code>f : Fin (m + n) → Float</code>, you can't simply call <code>f i</code> as <code>Fin (n + m)</code> is not <em>obviously</em> equal to <code>Fin (m + n)</code> because we need to invoke commutativity of addition on natural numbers. We will explain how to deal with this later; for now, let's have a look at the pooling layer.</p>
<p>Let's start with one dimension. A function that reduces the array size by two by taking the average of <code>x[2*i]</code> and <code>x[2*i+1]</code> is:</p>
<pre><code class="language-lean">def avgPool (x : Float^[n]) : Float^[n/2] := 
  ⊞ (i : Fin (n/2)) =&gt;
    let i₁ : Fin n := ⟨2*i.1, by omega⟩
    let i₂ : Fin n := ⟨2*i.1+1, by omega⟩
    0.5 * (x[i₁] + x[i₂])
</code></pre>
<p>Given the index of the new array <code>i : Fin (n/2)</code>, we need to produce indices <code>2*i.1</code> and <code>2*i.1+1</code> of the old vector, which have type <code>Fin n</code>. Recall that <code>Fin n</code> is just a pair of natural numbers and a proof that it is smaller than <code>n</code>. So far, we always omitted the proof with <code>sorry</code>, but we do not have to. Here, the proof can be easily done by calling the tactic <code>omega</code>, which is very good at proving index bounds. However, remember when you are writing a program, it is usually a good strategy to inspect all proofs, see if they are plausible, and omit them with <code>sorry</code>. Only once your program is capable of running, you can go back and start filling out the proofs. You can see this as an alternative way of debugging your program.</p>
<p>Beware! <code>Fin n</code> is endowed with modular arithmetic. Naively calling <code>2*i</code> would multiply <code>i</code> by two and perform modulo by <code>n/2</code>. We do not want that; we have to get the underlying natural number <code>i.1</code> and multiply then by two. For example:</p>
<pre><code class="language-lean">def i : Fin 10 := 6

#eval 2*i
#eval 2*i.1
</code></pre>
<p>One downside of the <code>avgPool</code> as written above is that if we call it multiple times, we get an array with an ugly type. For example, <code>avgPool (avgPool x)</code> has type <code>Float^[n/2/2]</code>. If we have a size that we already know is divisible by four, the <code>n/2/2</code> does not reduce. For <code>x : Float^[4*n]</code>, the term <code>avgPool (avgPool x)</code> has type <code>Float^[4*n/2/2]</code> and not <code>Float^[n]</code>.</p>
<p>You might attempt to solve this by writing the type of <code>avgPool</code> as:</p>
<pre><code class="language-lean">def avgPool (x : Float^[2*n]) : Float^[n] :=
  ⊞ (i : Fin n) =&gt;
    let i₁ : Fin (2*n) := ⟨2*i.1, by omega⟩
    let i₂ : Fin (2*n) := ⟨2*i.1+1, by omega⟩
    0.5 * (x[i₁] + x[i₂])
</code></pre>
<p>Unfortunately, this does not work. Lean's type checking is not smart enough to allow us to call <code>avgPool x</code> for <code>x : Float^[4*m]</code>. It can't figure out that <code>4*m = 2*(2*m)</code> and infer that <code>n = 2*m</code> when calling <code>avgPool</code>. We have to do something else.</p>
<p>The most flexible way of writing the <code>avgPool</code> function is as follows:</p>
<pre><code class="language-lean">def avgPool (x : Float^[n]) {m} (h : m = n/2 := by infer_var) : Float^[m] :=
  ⊞ (i : Fin m) =&gt;
    let i1 : Fin n := ⟨2*i.1, by omega⟩
    let i2 : Fin n := ⟨2*i.1+1, by omega⟩
    0.5 * (x[i1] + x[i2])
</code></pre>
<p>Here, the output dimension <code>m</code> is implicitly inferred from the proof <code>h : m = n/2</code>. Let's go step by step on what is going on.</p>
<p>When you call <code>avgPool x</code> for <code>x : Float^[4*k]</code>, the first argument is expected to have type <code>Float^[n]</code>. From this, Lean infers that <code>n = 4*k</code>. The next argument <code>{m}</code> is implicit, so Lean skips it for now as it is supposed to be inferred from the following arguments. Lastly, we have the argument <code>h : m = n/2</code>, which has the default value <code>by infer_var</code>. The tactic <code>infer_var</code> expects an expression with an undetermined variable, in our case <code>m</code>, and runs normalization on <code>n/2</code> and assigns the result to <code>m</code>. In this case, <code>4*k/2</code> gets simplified to <code>2*k</code>, and that is the final value of <code>m</code>.</p>
<p>You might be wondering what happens when <code>n</code> is odd. Because <code>n/2</code> performs natural division, for <code>x : Float^[2*n+1]</code>, calling <code>avgPool x</code> produces an array of type <code>Float^[n]</code>. If you want to prevent calling <code>avgPool</code> on arrays of odd length, you can simply modify the proof obligation to <code>(h : 2*m = n)</code>. This way, you require that <code>n</code> is even, and calling <code>avgPool x</code> with an odd-sized array <code>x</code> will produce an error.</p>
<p>To build a simple neural network, we need a two-dimensional version of the pooling layer:</p>
<pre><code class="language-lean">def avgPool2d
    (x : Float^[I,n₁,n₂]) {m₁ m₂}
    (h₁ : m₁ = n₁/2 := by infer_var)
    (h₂ : m₂ = n₂/2 := by infer_var) : Float^[I,m₁,m₂] :=
  ⊞ (ι : I) (i : Fin m₁) (j : Fin m₂) =&gt;
    let i₁ : Fin n₁ := ⟨2*i.1, by omega⟩
    let i₂ : Fin n₁ := ⟨2*i.1+1, by omega⟩
    let j₁ : Fin n₂ := ⟨2*j.1, by omega⟩
    let j₂ : Fin n₂ := ⟨2*j.1+1, by omega⟩
    0.5 * (x[ι,i₁,j₁] + x[ι,i₁,j₂] + x[ι,i₂,j₁] + x[ι,i₂,j₂])
</code></pre>
<h2 id="simple-neural-network"><a class="header" href="#simple-neural-network">Simple Neural Network</a></h2>
<p>We are almost ready to write a simple neural network. The only missing piece is the dense layer, which is just matrix multiplication followed by addition. We have already shown matrix multiplication previously, but it was only for multiplying by a normal matrix with <code>n</code> rows and <code>m</code> columns. A general dense layer takes a tensor <code>x</code> of any shape, treats it as a flat array of <code>m</code> elements, and multiplies that by an <code>n×m</code> matrix. Because our arrays allow indexing by an arbitrary type <code>I</code>, we do not need to perform this flattening explicitly and can just multiply by the matrix <code>Float^[n,I]</code>.</p>
<pre><code class="language-lean">def dense (n : Nat) (A : Float^[n,I]) (b : Float^[n]) (x : Float^[I]) : Float^[n] :=
  ⊞ (i : Fin n) =&gt; b[i] + ∑ j, A[i,j] * x[j]
</code></pre>
<p>Finally, we have all the necessary pieces, and we can implement a simple neural network.</p>
<pre><code class="language-lean">def nnet := fun (w₁,b₁,w₂,b₂,w₃,b₃) (x : Float^[28,28]) =&gt;
  x |&gt; resize3 1 28 28 (by decide)
    |&gt; conv2d 1 (Fin 8) w₁ b₁ 
    |&gt;.mapMono (fun x =&gt; max x 0)
    |&gt; avgPool2d
    |&gt; dense 30 w₂ b₂
    |&gt;.mapMono (fun x =&gt; max x 0)
    |&gt; dense 10 w₃ b₃
    |&gt; softMax 0.1
</code></pre>
<p>When we check the type of <code>nnet</code>, we get:</p>
<pre><code class="language-lean">Float^[8,1,[-1:1],[-1:1]] × Float^[8,28,28] × Float^[30,8,14,14] × Float^[30] × Float^[10,30] × Float^[10] → Float^[28,28] → Float^[10]
</code></pre>
<p>You can see that the type of weights is automatically inferred to be:</p>
<pre><code class="language-lean">Float^[8,1,[-1:1],[-1:1]] × Float^[8,28,28] × Float^[30,8,14,14] × Float^[30] × Float^[10,30] × Float^[10]
</code></pre>
<p>The input image has type <code>Float^[28,28]</code>, and the output is an array of ten elements <code>Float^[10]</code>. As you might have guessed from the dimensions, later in the book, we will train this network to classify handwritten digits from the MNIST database.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../working-with-arrays/basics.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../working-with-arrays/optimizing-arrays.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../working-with-arrays/basics.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../working-with-arrays/optimizing-arrays.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
    </body>
</html>
